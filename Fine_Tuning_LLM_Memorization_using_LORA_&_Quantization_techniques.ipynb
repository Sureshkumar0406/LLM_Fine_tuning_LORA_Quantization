{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sureshkumar0406/LLM_Fine_tuning_LORA_Quantization/blob/main/Fine_Tuning_LLM_Memorization_using_LORA_%26_Quantization_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ilUcxy2HDn6",
        "outputId": "fab3b91e-7403-4f00-87d7-baa55f5badea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G82GuO-tez",
        "outputId": "0418e2cd-2b17-4c3c-bbb6-ed3eb13dba15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version  # Check CUDA version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP1wiqbkWWH2",
        "outputId": "0ebf0e2a-f65d-44a1-eff0-e7e7d6b7c9cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --index-url https://download.pytorch.org/whl/cu121\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WjGgpnGjWbmW",
        "outputId": "ff9df998-9ebf-498a-b8e5-e2a776ad7388"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install necessary libraries & its dependencies\n",
        "\n",
        "!python -m pip install --upgrade pip -q\n",
        "!pip install transformers==4.38.1 -q -U\n",
        "!pip install bitsandbytes==0.42.0 -q -U\n",
        "!pip install peft==0.8.2 -q -U\n",
        "# !pip install flash-attn==2.5.5 -q -U\n",
        "!pip install datasets==2.17.1 -q -U\n",
        "!pip install scipy==1.12.0 -q -U\n",
        "!pip install trl==0.7.11 -q -U\n",
        "!pip install hf_transfer==0.1.5 -q -U\n",
        "!pip install huggingface_hub==0.20.3 -q -U\n",
        "!pip install wandb==0.16.3 -q -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-avQhnqUfvaQ",
        "outputId": "1bb2fbe6-4913-43d3-ecc3-c70bc2b4921f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m142.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.20.3 which is incompatible.\n",
            "accelerate 1.3.0 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.20.3 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.27.2 -q -U"
      ],
      "metadata": {
        "id": "XWaTciuO358L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install & Save Packages to Google Drive to aovid re installing after kernel or session restarts\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a directory in Google Drive for storing installed packages\n",
        "!mkdir -p /content/drive/MyDrive/colab_packages\n",
        "\n",
        "# # Install necessary packages\n",
        "# !pip install transformers datasets accelerate\n",
        "\n",
        "# Save installed packages to Google Drive\n",
        "!cp -r /usr/local/lib/python3.*/dist-packages /content/drive/MyDrive/colab_packages/\n",
        "\n",
        "\n",
        "#  # Reload Packages Without Reinstalling (After Restart)\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Link the saved packages from Google Drive to Colab's system path\n",
        "# !cp -r /content/drive/MyDrive/colab_packages/dist-packages /usr/local/lib/python3.*/\n",
        "\n",
        "# # Verify if packages are available\n",
        "# !pip list | grep transformers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwPdiEdneu1a",
        "outputId": "85694c07-1585-4075-a05c-55ebd98e6852"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define cache directory\n",
        "CACHE_DIR = \"/content/model_cache\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Cache directory set to: {CACHE_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88vuFnlGgKWA",
        "outputId": "1333af88-a6be-487b-a0a2-b0694dfae676"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache directory set to: /content/model_cache\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading th eMistral model for Extractive Question Answering Task\n",
        "\n",
        "model_id = \"openchat/openchat_3.5\""
      ],
      "metadata": {
        "id": "qt4B3xwNa4GF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install & Save Packages to Google Drive to aovid re installing after kernel or session restarts\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a directory in Google Drive for storing installed packages\n",
        "!mkdir -p /content/drive/MyDrive/colab_packages\n",
        "\n",
        "# Uninstall and reinstall transformers\n",
        "!pip uninstall transformers -y\n",
        "!pip install transformers\n",
        "\n",
        "# Save installed packages to Google Drive\n",
        "!cp -r /usr/local/lib/python3.*/dist-packages /content/drive/MyDrive/colab_packages/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        },
        "id": "bcrMPCTOhrDF",
        "outputId": "890654a2-67eb-4f4a-83b8-da8f6f8a64d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found existing installation: transformers 4.38.1\n",
            "Uninstalling transformers-4.38.1:\n",
            "  Successfully uninstalled transformers-4.38.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
            "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "Successfully installed huggingface-hub-0.28.1 tokenizers-0.21.0 transformers-4.48.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "transformers"
                ]
              },
              "id": "351240aebb2e48f092bd1962fb81dfc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "collapsed": true,
        "id": "WB9VC45QsEz3",
        "outputId": "677f64da-449a-4c04-89df-81e6414d5b10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.28.0)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Using cached transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.28.0\n",
            "    Uninstalling transformers-4.28.0:\n",
            "      Successfully uninstalled transformers-4.28.0\n",
            "Successfully installed tokenizers-0.21.0 transformers-4.48.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "d12c14df2a3d4374b39e03c9997094da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the Quantization to avoid memory issues\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Enables 4-bit quantization\n",
        "    bnb_4bit_use_double_quant=True,  # Enables double quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Uses NormalFloat4 for quantization\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    load_in_8bit_fp32_cpu_offload=True\n",
        ")"
      ],
      "metadata": {
        "id": "fzu9j5Kt-2cg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Model with  Quantization\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    # rope_scaling={\"type\": \"linear\", \"factor\": 2.0},\n",
        "    device_map='auto',\n",
        "    # trust_remote_code=False,\n",
        "    torch_dtype=torch.float16,\n",
        "    cache_dir = CACHE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "c1e9088a4f324e8d8cb1af0cca33662a",
            "31e492c832284c8db0ff9565429ea773",
            "f3b55eb347b34c929b4d38eadccd9bef",
            "3e8e4db6711c4644a07105b0c9c78a25",
            "1484dfce0fd248fcb09999efbc31bf82",
            "ae05589cfa924a539a8ade0a544c1f41",
            "a3437e178b5b475bb7a0a0eabc34eac4",
            "17876e9c41df4859879d416b6ff86cea",
            "26838ef09f444faa8ca9f40a020deaf9",
            "87e6051d71e44bdca60bdbd170a8bce6",
            "8279474e9ed9463399dea7a3d52f8d5b"
          ]
        },
        "id": "J5t2tMmA-6OV",
        "outputId": "34936d01-5f30-4fc5-a7a9-7b3dcc88948e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1e9088a4f324e8d8cb1af0cca33662a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True,trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-BYr2zZa4hy",
        "outputId": "fd84f617-9419-489c-ddd9-746ac455d8c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Save tokenizer to a directory\n",
        "tokenizer.save_pretrained(\"./saved_tokenizer\")\n",
        "\n",
        "\n",
        "\n",
        "# #ReLoad tokenizer from saved directory in case if the server or session got restarted\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"./saved_tokenizer\")\n",
        "\n",
        "# print(\"Tokenizer loaded successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i31qH_p4m79",
        "outputId": "81b24dec-1889-496c-ad7f-d2c179c529d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./saved_tokenizer/tokenizer_config.json',\n",
              " './saved_tokenizer/special_tokens_map.json',\n",
              " './saved_tokenizer/tokenizer.model',\n",
              " './saved_tokenizer/added_tokens.json',\n",
              " './saved_tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check there are no parameters overflowing onto cpu (meta).\n",
        "for n, p in model.named_parameters():\n",
        "    if p.device.type == \"meta\":\n",
        "        print(f\"{n} is on meta!\")"
      ],
      "metadata": {
        "id": "9AL5CeeC76iw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.max_position_embeddings)\n",
        "print(model.config.eos_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixjj5grg7-yQ",
        "outputId": "dfadebfd-b708-4afc-b2f2-1e7acfb41f5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8192\n",
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "hFD_l1rFWtNh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "collapsed": true,
        "id": "R54_kbB-8wRF",
        "outputId": "b9427d3c-2f3d-41b2-f442-f861911e514d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.17.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
            "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
            "Installing collected packages: huggingface_hub, datasets\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.17.1\n",
            "    Uninstalling datasets-2.17.1:\n",
            "      Successfully uninstalled datasets-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.3.0 huggingface_hub-0.28.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets",
                  "huggingface_hub"
                ]
              },
              "id": "2be7b6bc4b0b494e84bd5589caecc6e1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset=\"Trelis/touch-rugby-rules-memorisation\"\n",
        "\n",
        "data = load_dataset(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "BGuy3PF08NxP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfXu6vSD88y5",
        "outputId": "485e4286-9ab0-4df7-bdf4-0e312536d80d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': '{\"messages\": [{\"role\": \"user\", \"content\": \"What edition of the Touch Football rules was presented by Touch Football Australia at the Federation of International Touch Paris Convention in October 2019?\"}, {\"role\": \"assistant\", \"content\": \"The TFA 8th Edition Playing Rules were presented by Touch Football Australia at the Federation of International Touch Paris Convention in October 2019.\"}, {\"role\": \"user\", \"content\": \"Who resolved to adopt the TFA 8th Edition Playing Rules as the 5th Edition FIT Playing Rules internationally?\"}, {\"role\": \"assistant\", \"content\": \"The FIT Board resolved to adopt the TFA 8th Edition Playing Rules as the 5th Edition FIT Playing Rules to be used across all levels of the game internationally.\"}, {\"role\": \"user\", \"content\": \"Are National Touch Associations (NTAs) required to pay royalties to use the newly developed rules presented by Touch Football Australia?\"}, {\"role\": \"assistant\", \"content\": \"No, Touch Football Australia offered FIT and all National Touch Associations (NTAs) royalty-free rights to use the newly developed rules.\"}, {\"role\": \"user\", \"content\": \"What is the importance of consistency in the application of the rules of the game according to FIT?\"}, {\"role\": \"assistant\", \"content\": \"While consistency in the application of the rules of the game is important, FIT encourages its members to offer features in local competition rules to ensure that all participants enjoy a high-quality experience.\"}, {\"role\": \"user\", \"content\": \"Can NTAs or their authorized competition providers have different match conditions compared to the FIT Playing Rules?\"}, {\"role\": \"assistant\", \"content\": \"Yes, any adaptation of or alterations to the Rules for local competitions should be clearly articulated in relevant competition guidelines and be readily available for players, coaches, and referees alike.\"}]}'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from the first row of 'test' in data\n",
        "text = data['train'][0]['messages']\n",
        "\n",
        "# Make sure to define or import the tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define model_id here\n",
        "model_id = \"openchat/openchat_3.5\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "# Decode back to text\n",
        "decoded_text = tokenizer.decode(tokens)\n",
        "\n",
        "# Print tokens and decoded text\n",
        "print(\"Token IDs:\", tokens)\n",
        "print(\"Decoded Text:\", decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA-VrpAn9BlO",
        "outputId": "743be7e5-9966-40d9-82a8-a5c1e792c951"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [1, 9830, 16167, 1264, 733, 6799, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 3195, 12065, 302, 272, 25086, 13474, 5879, 403, 7567, 486, 25086, 13474, 6664, 438, 272, 22520, 302, 5440, 25086, 5465, 23717, 297, 4527, 28705, 28750, 28734, 28740, 28774, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 1014, 320, 3120, 28705, 28783, 362, 20356, 6879, 288, 24759, 654, 7567, 486, 25086, 13474, 6664, 438, 272, 22520, 302, 5440, 25086, 5465, 23717, 297, 4527, 28705, 28750, 28734, 28740, 28774, 611, 881, 9830, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 11447, 15813, 298, 8799, 272, 320, 3120, 28705, 28783, 362, 20356, 6879, 288, 24759, 390, 272, 28705, 28782, 362, 20356, 12630, 6879, 288, 24759, 17861, 578, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 1014, 12630, 9217, 15813, 298, 8799, 272, 320, 3120, 28705, 28783, 362, 20356, 6879, 288, 24759, 390, 272, 28705, 28782, 362, 20356, 12630, 6879, 288, 24759, 298, 347, 1307, 2673, 544, 6157, 302, 272, 2039, 17861, 578, 611, 881, 9830, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 5985, 3610, 25086, 6097, 697, 325, 8124, 2198, 28731, 3030, 298, 2136, 712, 28724, 23771, 298, 938, 272, 12486, 6202, 5879, 7567, 486, 25086, 13474, 6664, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 2501, 28725, 25086, 13474, 6664, 6373, 12630, 304, 544, 3610, 25086, 6097, 697, 325, 8124, 2198, 28731, 13753, 884, 28733, 3669, 4495, 298, 938, 272, 12486, 6202, 5879, 611, 881, 9830, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 3195, 349, 272, 9545, 302, 23074, 297, 272, 4993, 302, 272, 5879, 302, 272, 2039, 4771, 298, 12630, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 23475, 23074, 297, 272, 4993, 302, 272, 5879, 302, 272, 2039, 349, 2278, 28725, 12630, 6191, 1291, 871, 3338, 298, 2405, 4190, 297, 1862, 8995, 5879, 298, 5407, 369, 544, 12850, 3555, 264, 1486, 28733, 14817, 2659, 611, 881, 9830, 9606, 1264, 345, 1838, 548, 345, 3789, 1264, 345, 6325, 26577, 2198, 442, 652, 23243, 8995, 14314, 506, 1581, 2918, 4331, 6731, 298, 272, 12630, 6879, 288, 24759, 1110, 881, 9830, 9606, 1264, 345, 489, 11143, 548, 345, 3789, 1264, 345, 5613, 28725, 707, 25661, 302, 442, 13496, 697, 298, 272, 24759, 354, 1862, 3440, 2065, 1023, 347, 6315, 22187, 6432, 297, 8598, 8995, 18392, 304, 347, 22058, 2632, 354, 5117, 28725, 25360, 28725, 304, 1792, 397, 274, 21892, 611, 28752, 9205]\n",
            "Decoded Text: <s> {\"messages\": [{\"role\": \"user\", \"content\": \"What edition of the Touch Football rules was presented by Touch Football Australia at the Federation of International Touch Paris Convention in October 2019?\"}, {\"role\": \"assistant\", \"content\": \"The TFA 8th Edition Playing Rules were presented by Touch Football Australia at the Federation of International Touch Paris Convention in October 2019.\"}, {\"role\": \"user\", \"content\": \"Who resolved to adopt the TFA 8th Edition Playing Rules as the 5th Edition FIT Playing Rules internationally?\"}, {\"role\": \"assistant\", \"content\": \"The FIT Board resolved to adopt the TFA 8th Edition Playing Rules as the 5th Edition FIT Playing Rules to be used across all levels of the game internationally.\"}, {\"role\": \"user\", \"content\": \"Are National Touch Associations (NTAs) required to pay royalties to use the newly developed rules presented by Touch Football Australia?\"}, {\"role\": \"assistant\", \"content\": \"No, Touch Football Australia offered FIT and all National Touch Associations (NTAs) royalty-free rights to use the newly developed rules.\"}, {\"role\": \"user\", \"content\": \"What is the importance of consistency in the application of the rules of the game according to FIT?\"}, {\"role\": \"assistant\", \"content\": \"While consistency in the application of the rules of the game is important, FIT encourages its members to offer features in local competition rules to ensure that all participants enjoy a high-quality experience.\"}, {\"role\": \"user\", \"content\": \"Can NTAs or their authorized competition providers have different match conditions compared to the FIT Playing Rules?\"}, {\"role\": \"assistant\", \"content\": \"Yes, any adaptation of or alterations to the Rules for local competitions should be clearly articulated in relevant competition guidelines and be readily available for players, coaches, and referees alike.\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model and lists which parameters are trainable.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    non_trainable_params = 0\n",
        "    all_params = 0\n",
        "\n",
        "    print(\"Trainable Parameters:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "            print(f\"  {name}\")\n",
        "        else:\n",
        "            non_trainable_params += param.numel()\n",
        "\n",
        "    print(\"\\nNon-Trainable Parameters:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            print(f\"  {name}\")\n",
        "\n",
        "    print(\n",
        "        f\"\\nSummary:\\n  Trainable params: {trainable_params}\\n  Non-Trainable params: {non_trainable_params}\\n  All params: {all_params}\\n  Trainable%: {100 * trainable_params / all_params}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "GGAd5hIG9u39"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eKjRH2LQl70i",
        "outputId": "fe83ed0a-74e3-42d0-92c9-984b7152ec41"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.42.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.42.0\n",
            "    Uninstalling bitsandbytes-0.42.0:\n",
            "      Successfully uninstalled bitsandbytes-0.42.0\n",
            "Successfully installed bitsandbytes-0.45.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'"
      ],
      "metadata": {
        "id": "g3ttGozgmgMz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVS4Fk0GmAAw",
        "outputId": "58ac5166-67b3-4c17-a44a-a5137bebf466"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++ BUG REPORT INFORMATION ++++++++++++++++++\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\n",
            "CUDA specs: CUDASpecs(highest_compute_capability=(8, 0), cuda_version_string='124', cuda_version_tuple=(12, 4))\n",
            "PyTorch settings found: CUDA_VERSION=124, Highest Compute Capability: (8, 0).\n",
            "To manually override the PyTorch CUDA version please see: https://github.com/TimDettmers/bitsandbytes/blob/main/docs/source/nonpytorchcuda.mdx\n",
            "The directory listed in your path is found to be non-existent: /sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\n",
            "The directory listed in your path is found to be non-existent: //172.28.0.1\n",
            "The directory listed in your path is found to be non-existent: 8013\n",
            "The directory listed in your path is found to be non-existent: //colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-pnfz71k1yh4k --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true --enable_kernel_event_logging=true \n",
            "The directory listed in your path is found to be non-existent: /datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\n",
            "The directory listed in your path is found to be non-existent: /env/python\n",
            "The directory listed in your path is found to be non-existent: //ipykernel.pylab.backend_inline\n",
            "CUDA SETUP: WARNING! CUDA runtime files not found in any environmental path.\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "Checking that the library is importable and CUDA is callable...\n",
            "SUCCESS!\n",
            "Installation was successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up LORA for the model fine tuning\n",
        "\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "7dcYxCdQ9xPp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsH0L4G1nm47",
        "outputId": "48b73389-0442-4872-9c45-344ae9745c4c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32002, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): MistralRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9gpswewnqY6",
        "outputId": "b2a07c6a-bac5-4d5d-a47f-c864db598036"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters:\n",
            "  base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
            "\n",
            "Non-Trainable Parameters:\n",
            "  base_model.model.model.embed_tokens.weight\n",
            "  base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.0.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.0.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.0.input_layernorm.weight\n",
            "  base_model.model.model.layers.0.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.1.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.1.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.1.input_layernorm.weight\n",
            "  base_model.model.model.layers.1.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.2.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.2.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.2.input_layernorm.weight\n",
            "  base_model.model.model.layers.2.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.3.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.3.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.3.input_layernorm.weight\n",
            "  base_model.model.model.layers.3.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.4.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.4.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.4.input_layernorm.weight\n",
            "  base_model.model.model.layers.4.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.5.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.5.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.5.input_layernorm.weight\n",
            "  base_model.model.model.layers.5.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.6.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.6.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.6.input_layernorm.weight\n",
            "  base_model.model.model.layers.6.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.7.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.7.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.7.input_layernorm.weight\n",
            "  base_model.model.model.layers.7.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.8.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.8.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.8.input_layernorm.weight\n",
            "  base_model.model.model.layers.8.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.9.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.9.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.9.input_layernorm.weight\n",
            "  base_model.model.model.layers.9.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.10.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.10.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.10.input_layernorm.weight\n",
            "  base_model.model.model.layers.10.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.11.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.11.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.11.input_layernorm.weight\n",
            "  base_model.model.model.layers.11.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.12.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.12.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.12.input_layernorm.weight\n",
            "  base_model.model.model.layers.12.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.13.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.13.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.13.input_layernorm.weight\n",
            "  base_model.model.model.layers.13.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.14.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.14.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.14.input_layernorm.weight\n",
            "  base_model.model.model.layers.14.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.15.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.15.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.15.input_layernorm.weight\n",
            "  base_model.model.model.layers.15.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.16.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.16.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.16.input_layernorm.weight\n",
            "  base_model.model.model.layers.16.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.17.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.17.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.17.input_layernorm.weight\n",
            "  base_model.model.model.layers.17.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.18.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.18.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.18.input_layernorm.weight\n",
            "  base_model.model.model.layers.18.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.19.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.19.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.19.input_layernorm.weight\n",
            "  base_model.model.model.layers.19.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.20.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.20.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.20.input_layernorm.weight\n",
            "  base_model.model.model.layers.20.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.21.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.21.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.21.input_layernorm.weight\n",
            "  base_model.model.model.layers.21.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.22.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.22.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.22.input_layernorm.weight\n",
            "  base_model.model.model.layers.22.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.23.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.23.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.23.input_layernorm.weight\n",
            "  base_model.model.model.layers.23.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.24.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.24.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.24.input_layernorm.weight\n",
            "  base_model.model.model.layers.24.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.25.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.25.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.25.input_layernorm.weight\n",
            "  base_model.model.model.layers.25.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.26.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.26.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.26.input_layernorm.weight\n",
            "  base_model.model.model.layers.26.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.27.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.27.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.27.input_layernorm.weight\n",
            "  base_model.model.model.layers.27.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.28.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.28.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.28.input_layernorm.weight\n",
            "  base_model.model.model.layers.28.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.29.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.29.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.29.input_layernorm.weight\n",
            "  base_model.model.model.layers.29.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.30.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.30.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.30.input_layernorm.weight\n",
            "  base_model.model.model.layers.30.post_attention_layernorm.weight\n",
            "  base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight\n",
            "  base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight\n",
            "  base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight\n",
            "  base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight\n",
            "  base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight\n",
            "  base_model.model.model.layers.31.mlp.up_proj.base_layer.weight\n",
            "  base_model.model.model.layers.31.mlp.down_proj.base_layer.weight\n",
            "  base_model.model.model.layers.31.input_layernorm.weight\n",
            "  base_model.model.model.layers.31.post_attention_layernorm.weight\n",
            "  base_model.model.model.norm.weight\n",
            "  base_model.model.lm_head.weight\n",
            "\n",
            "Summary:\n",
            "  Trainable params: 20971520\n",
            "  Non-Trainable params: 7241748480\n",
            "  All params: 7262720000\n",
            "  Trainable%: 0.28875572788156506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)\n",
        "print(tokenizer.vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddp6bBFHnyDh",
        "outputId": "526bcb4c-e7e8-436a-84ea-2eb71b93babd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaTokenizerFast(name_or_path='openchat/openchat_3.5', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|end_of_turn|>', 'unk_token': '<unk>', 'additional_special_tokens': ['<|end_of_turn|>', '<|pad_0|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32000: AddedToken(\"<|end_of_turn|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32001: AddedToken(\"<|pad_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n",
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the chat template\n",
        "messages=[\n",
        "    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"},\n",
        "    { 'role': 'assistant', 'content': \"here you are.\"},\n",
        "    { 'role': 'user', 'content': \"great.\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yEJJNrZn1Oq",
        "outputId": "3c29ed0e-a535-4cb2-aa43-56f61b1bf1df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>GPT4 Correct User: write a quick sort algorithm in python.<|end_of_turn|>GPT4 Correct Assistant: here you are.<|end_of_turn|>GPT4 Correct User: great.<|end_of_turn|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up padding. Lets set the pad token to <pad>, if not <|pad|>, if not <unk> if <unk> is in the tokenizer OR set it to the EOS token.\n",
        "if '<pad>' in tokenizer.get_vocab():\n",
        "    print('<pad> token is in the tokenizer. Using <pad> for pad')\n",
        "\n",
        "    tokenizer.pad_token = '<pad>'\n",
        "elif '<|pad|>' in tokenizer.get_vocab():\n",
        "    print('<|pad|> token is in the tokenizer. Using <|pad|> for pad')\n",
        "\n",
        "    tokenizer.pad_token = '<|pad|>'\n",
        "elif '<unk>' in tokenizer.get_vocab():\n",
        "    print('<unk> token is in the tokenizer. Using unk for pad')\n",
        "\n",
        "    tokenizer.pad_token = '<unk>'\n",
        "else:\n",
        "    print(f'Using EOS token, {tokenizer.eos_token}, for padding. WARNING, this may not be ideal for chat fine-tuning models.')\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faWSdTDOn25p",
        "outputId": "20a63404-b5aa-40e0-b320-9072b239d629"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> token is in the tokenizer. Using unk for pad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update pad token id in model and its config\n",
        "model.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Check if they are equal\n",
        "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
        "\n",
        "# Print the pad token ids\n",
        "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
        "print('Model pad token ID:', model.pad_token_id)\n",
        "print('Model config pad token ID:', model.config.pad_token_id)\n",
        "print('Number of tokens now in tokenizer:', tokenizer.vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EU_PNkwn3LX",
        "outputId": "14a17021-de34-4fde-db63-7c7db45bac10"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer pad token ID: 0\n",
            "Model pad token ID: 0\n",
            "Model config pad token ID: 0\n",
            "Number of tokens now in tokenizer: 32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Special tokens map:\", tokenizer.special_tokens_map)\n",
        "# print(\"All special tokens:\", tokenizer.all_special_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIxrfDtdoGNh",
        "outputId": "ece771ea-af52-4639-d0f0-cfa441b47b98"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special tokens map: {'bos_token': '<s>', 'eos_token': '<|end_of_turn|>', 'unk_token': '<unk>', 'pad_token': '<unk>', 'additional_special_tokens': ['<|end_of_turn|>', '<|pad_0|>']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define streaming fucntion & evaluate it\n",
        "\n",
        "from transformers import TextStreamer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import gc  # import Python's garbage collection module\n",
        "\n",
        "# Define a stream\n",
        "def stream(user_prompt, model_type, tokenizer, checkpoint=''):\n",
        "\n",
        "    if model_type == 'base':\n",
        "        eval_model = model\n",
        "    elif model_type == 'fine-tuned':\n",
        "        eval_model = PeftModel.from_pretrained(model, checkpoint)  # Assuming PeftModel is the intended class\n",
        "        eval_model = eval_model.to(\"cuda\")\n",
        "\n",
        "        for n, p in eval_model.named_parameters():\n",
        "            if p.device.type == \"cpu\":\n",
        "                print(f\"{n} is on cpu!\")\n",
        "\n",
        "    else:\n",
        "        print('You must set the model_type to base or fine-tuned')\n",
        "        exit()  # or raise an exception\n",
        "\n",
        "    # print(f'Proceeding to inference with peft adapters from {checkpoint}')\n",
        "\n",
        "    eval_model.config.use_cache = True\n",
        "\n",
        "    messages=[\n",
        "        { 'role': 'user', 'content': f\"{user_prompt.strip()}\"},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer([inputs], return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "    if \"token_type_ids\" in inputs:\n",
        "        del inputs[\"token_type_ids\"]\n",
        "\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    print(f'eval_model is on: {next(eval_model.parameters()).device}')  # Debug line\n",
        "    print(f'input_ids are on: {inputs[\"input_ids\"].device}')  # Debug line\n",
        "\n",
        "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
        "    # _ = eval_model.generate(**inputs, streamer=streamer)\n",
        "    _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=100, do_sample=False, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,num_beams=1 )\n",
        "\n",
        "    # Clear GPU cache and run garbage collection\n",
        "    torch.cuda.empty_cache()  # Clear GPU cache\n",
        "    gc.collect()  # Run garbage collection\n",
        "\n",
        "\n",
        "def evaluation(model_type, tokenizer, checkpoint=''):\n",
        "    questions = [\n",
        "        \"In the context of Touch Rugby International Playing Rules 2020, what is the purpose of the Dead Ball Line?\", #copied from the test data set to ensure training is working\n",
        "        \"How many players are on the field on each team in touch rugby?\",\n",
        "        \"In touch rugby, does a forward pass result in a roll ball, a scrum, or something else?\",\n",
        "        \"In touch rugby, how many metres must the defending team retreat after a touch?\",\n",
        "        \"In touch rugby, how many substitutions are allowed during a game?\",\n",
        "        \"In touch rugby, how long is half time?\",\n",
        "        \"In touch rugby, how does the game commence?\",\n",
        "        \"In touch rugby, how many metres must defenders retreat when there is a penalty? Is the same as after a touch is made?\",\n",
        "        \"In touch rugby, how many touches is a team entitled to prior to a change in possession?\",\n",
        "        \"In touch rugby, what happens if a player makes a pass after a touch has been made?\",\n",
        "        \"In touch rugby, how many points is a try worth?\"\n",
        "    ]\n",
        "\n",
        "    answers = [\n",
        "        \"The Dead Ball Line marks the end boundaries of the field of play and indicates when the ball is out of play.\",\n",
        "        \"6 players.\",\n",
        "        \"Penalty.\",\n",
        "        \"7 metres.\",\n",
        "        \"There is no limit.\",\n",
        "        \"5 minutes.\",\n",
        "        \"The game begins with a tap on the halfway line.\",\n",
        "        \"10 metres.\",\n",
        "        \"Possession changes on the sixth (6th) touch.\",\n",
        "        \"The defending team gains possession and a penalty.\",\n",
        "        \"1 point.\"\n",
        "    ]\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        stream(question, model_type, tokenizer, checkpoint)\n",
        "        print(\"Correct Answer:\", answer)\n",
        "        print('\\n\\n')"
      ],
      "metadata": {
        "id": "AcN786hJoJYw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.generation_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flrM-ll_oMCR",
        "outputId": "0e41ffec-1c0d-4ef3-9e09-8902e27ac075"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the base pre-trained model\n",
        "\n",
        "evaluation(\"base\", tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UfvU3ApoOkL",
        "outputId": "10931982-5bdd-4585-e700-0d607adcdcf8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> GPT4 Correct User: In the context of Touch Rugby International Playing Rules 2020, what is the purpose of the Dead Ball Line?<|end_of_turn|> GPT4 Correct Assistant: The Dead Ball Line in Touch Rugby International Playing Rules 2020 serves as a boundary line on the field. When a ball goes beyond this line, it is considered out of play. The team that last touched the ball before it went out of play will be awarded a scrum or a line-out, depending on the situation. The Dead Ball Line helps to maintain the flow of the game and ensures that the ball remains in play within the designated area.<|end_of_turn|>\n",
            "Correct Answer: The Dead Ball Line marks the end boundaries of the field of play and indicates when the ball is out of play.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: How many players are on the field on each team in touch rugby?<|end_of_turn|> GPT4 Correct Assistant: In "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "touch rugby, each team has 12 players on the field at any given time. Therefore, the answer is 12.<|end_of_turn|>\n",
            "Correct Answer: 6 players.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, does a forward pass result in a roll ball, a scrum, or something else?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, a forward pass results in a roll ball.<|end_of_turn|>\n",
            "Correct Answer: Penalty.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many metres must the defending team retreat after a touch?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, the defending team must retreat 5 metres after a touch. Therefore, the answer is 5.<|end_of_turn|>\n",
            "Correct Answer: 7 metres.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many substitutions are allowed during a game?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, there is no limit to the number of substitutions that can be made during a game. Teams can make substitutions at any time, as long as they have not exceeded the maximum number of players allowed on the field, which is typically 12 players per team. Therefore, the answer is unlimited.<|end_of_turn|>\n",
            "Correct Answer: There is no limit.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how long is half time?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, half time typically lasts for 5-10 minutes. However, the exact duration of half time can vary depending on the specific rules and regulations of the competition or league in which the game is being played. Therefore, the answer cannot be determined with absolute certainty.<|end_of_turn|>\n",
            "Correct Answer: 5 minutes.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how does the game commence?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, the game commences with a kick-off. One team kicks the ball from the center of the field, and the other team attempts to catch or touch the ball. The team that successfully catches or touches the ball then takes possession and begins to move the ball down the field, attempting to score points by touching the ball down behind the opponent's goal line.<|end_of_turn|>\n",
            "Correct Answer: The game begins with a tap on the halfway line.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many metres must defenders retreat when there is a penalty? Is the same as after a touch is made?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, the distance that defenders must retreat when there is a penalty is 5 meters. This is also the same distance that defenders must retreat after a touch is made. Therefore, the answer is True.<|end_of_turn|>\n",
            "Correct Answer: 10 metres.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many touches is a team entitled to prior to a change in possession?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, a team is entitled to a maximum of 6 touches prior to a change in possession. Therefore, the answer is 6.<|end_of_turn|>\n",
            "Correct Answer: Possession changes on the sixth (6th) touch.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, what happens if a player makes a pass after a touch has been made?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, if a player makes a pass after a touch has been made, the pass is considered illegal and the player who made the pass is deemed to be \"in touch\" and must retreat behind the touchline. This is because in touch rugby, once a touch has been made, the player must immediately retreat behind the touchline and cannot continue playing with the ball. If the player fails to retreat, the opposing team is awarded a free pass or a scrum, depending on the\n",
            "Correct Answer: The defending team gains possession and a penalty.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many points is a try worth?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, a try is worth 1 point.<|end_of_turn|>\n",
            "Correct Answer: 1 point.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.31.0\n",
        "!pip install trl==0.7.11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "kIemZDwBpi__",
        "outputId": "e829d3be-9711-4d01-9d88-68f0d23eadc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.31.0\n",
            "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.1.31)\n",
            "Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "5c47f100127841ea866589e7bcab07ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl==0.7.11 in /usr/local/lib/python3.11/dist-packages (0.7.11)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.7.11) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.7.11) (4.31.0)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.11/dist-packages (from trl==0.7.11) (1.26.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from trl==0.7.11) (0.27.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from trl==0.7.11) (2.17.1)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl==0.7.11) (0.9.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.7.11) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.7.11) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.7.11) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.7.11) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.7.11) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.7.11) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.7.11) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.7.11) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.7.11) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.7.11) (4.67.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.7.11) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.7.11) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.7.11) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.7.11) (4.4.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->trl==0.7.11) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.7.11) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.7.11) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.7.11) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.7.11) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.7.11) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.7.11) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.7.11) (3.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.7.11) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.7.11) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.7.11) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.7.11) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.7.11) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.7.11) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.7.11) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.7.11) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.7.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.7.11) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.7.11) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->trl==0.7.11) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.7.11) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.7.11) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.7.11) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.11) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_id.split(\"/\")[-1]\n",
        "dataset_name = dataset.split(\"/\")[-1]\n",
        "\n",
        "epochs=1\n",
        "context_length = 512\n",
        "grad_accum=1\n",
        "batch_size=1\n",
        "fine_tune_tag='touch-rugby-rules'\n",
        "save_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{context_length}_length-{fine_tune_tag}'\n",
        "print(save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpKzjtQarSkn",
        "outputId": "926afab6-7195-474b-f859-cea0d8a8c43b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub==0.25.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "collapsed": true,
        "id": "0UKhr0vAvIjD",
        "outputId": "77a0f492-b91a-4fd4-8cb5-a65e7a64c5cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub==0.25.2\n",
            "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.25.2) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.25.2) (2023.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.25.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.25.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.25.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.25.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.25.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.25.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.25.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.25.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.25.2) (2025.1.31)\n",
            "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
            "Installing collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.28.1\n",
            "    Uninstalling huggingface-hub-0.28.1:\n",
            "      Successfully uninstalled huggingface-hub-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "8901c4aab57b4461b6a6638bfcf6a33c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)  # Should be 4.38.1 or higher\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPTHX27n1so0",
        "outputId": "4e72f5df-8489-408f-b5c4-fbf1af6a7fc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from trl import SFTTrainer\n",
        "import transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibKDk-6mrJzf",
        "outputId": "fe18b401-0f6a-4fdb-ccf7-a1c58d2e035a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up trianing argumaents & trian the model\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    # peft_config=peft_config, #comment out if passing a peft model directly as 'model'\n",
        "    dataset_text_field=\"messages\",\n",
        "    max_seq_length=context_length,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    eval_dataset=data[\"test\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        # max_steps=1, # comment this out after the first time you run. This is for testing!\n",
        "        save_steps=10, ### MAKE SURE TO CHECK THIS VALUE IS GOOD FOR YOUR RUN!\n",
        "        logging_steps=1,\n",
        "        num_train_epochs=epochs,\n",
        "        output_dir=save_dir,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        do_eval=True,\n",
        "        eval_steps=0.2,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=grad_accum,\n",
        "        log_level=\"debug\",\n",
        "        bf16=True,\n",
        "        max_grad_norm=0.3,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        hub_private_repo=True,\n",
        "        warmup_ratio=0.03, # optional, may help stability at the start of training. Not required for simple fine-tunes.\n",
        "        optim=\"adamw_torch\", #comment out for LoRA +\n",
        "        learning_rate=1e-4, #comment out for LoRA +\n",
        "    ),\n",
        "    #callbacks=[logging_callback],  # Add custom callback here\n",
        "    # optimizers=(optimizer, None),  # Comment in for LoRA+\n",
        "    # neftune_noise_alpha=5 # Add in noise to embeddings to improve performance!\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "9ed7922d778044228575174731187884",
            "86ee9c6e5d2b4eea80fdc1db1a37fadf",
            "a4e717033ec34b8c8972db133f6924ff",
            "e6e86e8eb51949c5b415481c6b804581",
            "705db3c14c1a42869d754c3e8f78152f",
            "0d21393569b14ab5ade2aa999f65be3d",
            "7feaaeed92d649538f28ce49933651d0",
            "ec2a0dcf29c244238757cf8dc95abf76",
            "c791b05dcb564bc19faedf3714bdd445",
            "0f0a207530454576837139c97e13a2f9",
            "55f16a27eec94785b7cb487b95d0c30f"
          ]
        },
        "id": "bWJDLJl32eg5",
        "outputId": "6df7b2a7-5f28-4b65-ae8c-0f7207615a91"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ed7922d778044228575174731187884"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "Using auto half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zOKn02iS3aqJ",
        "outputId": "c25f02d2-4cc0-46e8-9c08-6d3a67b5e760"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Currently training with a batch size of: 1\n",
            "***** Running training *****\n",
            "  Num examples = 303\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 303\n",
            "  Number of trainable parameters = 20,971,520\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msureshkumar-sekar86\u001b[0m (\u001b[33msureshkumar-sekar86-virtusa\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.19.6 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250217_082433-s1yqm0td</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sureshkumar-sekar86-virtusa/huggingface/runs/s1yqm0td' target=\"_blank\">graceful-snow-9</a></strong> to <a href='https://wandb.ai/sureshkumar-sekar86-virtusa/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sureshkumar-sekar86-virtusa/huggingface' target=\"_blank\">https://wandb.ai/sureshkumar-sekar86-virtusa/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sureshkumar-sekar86-virtusa/huggingface/runs/s1yqm0td' target=\"_blank\">https://wandb.ai/sureshkumar-sekar86-virtusa/huggingface/runs/s1yqm0td</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='303' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [303/303 03:21, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.952400</td>\n",
              "      <td>0.838154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.608200</td>\n",
              "      <td>0.701534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.337900</td>\n",
              "      <td>0.638882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.480100</td>\n",
              "      <td>0.601945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-10\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-10/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-20\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-20/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-30\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-30/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-40\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-40/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-50\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-50/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-60\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-60/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 60\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-70\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-70/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-80\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-80/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-90\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-90/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-100\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-100/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-110\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-110/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-110/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-120\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-120/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 60\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-130\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-130/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-140\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-140/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-150\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-150/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-160\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-160/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-160/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-170\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-170/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-170/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-180\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-180/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-180/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 60\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-190\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-190/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-190/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-200\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-200/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-210\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-210/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-210/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-220\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-220/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-220/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-230\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-230/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-230/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-240\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-240/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-240/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 60\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-250\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-250/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-260\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-260/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-260/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-270\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-270/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-270/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-280\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-280/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-280/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-290\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-290/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-290/special_tokens_map.json\n",
            "Saving model checkpoint to ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-300\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openchat--openchat_3.5/snapshots/0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"_name_or_path\": \"imone/Mistral_7B_with_EOT_token\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32002\n",
            "}\n",
            "\n",
            "tokenizer config file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in ./results/openchat_3.5_touch-rugby-rules-memorisation_1_epochs_512_length-touch-rugby-rules/tmp-checkpoint-300/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=303, training_loss=0.6537407050923546, metrics={'train_runtime': 210.5409, 'train_samples_per_second': 1.439, 'train_steps_per_second': 1.439, 'total_flos': 4530759296581632.0, 'train_loss': 0.6537407050923546, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model post training\n",
        "\n",
        "evaluation(\"base\", tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhO_Zw3gCbyf",
        "outputId": "8aae2fb5-045e-457b-a137-e4e8d0a156ee"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In the context of Touch Rugby International Playing Rules 2020, what is the purpose of the Dead Ball Line?<|end_of_turn|> GPT4 Correct Assistant: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Dead Ball Line is the line where the ball is considered out of play. It is located at the end boundaries of the Field of Play.<|end_of_turn|>\n",
            "Correct Answer: The Dead Ball Line marks the end boundaries of the field of play and indicates when the ball is out of play.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: How many players are on the field on each team in touch rugby?<|end_of_turn|> GPT4 Correct Assistant: Each team has six players on the field at any given time in touch rugby.<|end_of_turn|>\n",
            "Correct Answer: 6 players.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, does a forward pass result in a roll ball, a scrum, or something else?<|end_of_turn|> GPT4 Correct Assistant: A forward pass results in a roll ball in touch rugby.<|end_of_turn|>\n",
            "Correct Answer: Penalty.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many metres must the defending team retreat after a touch?<|end_of_turn|> GPT4 Correct Assistant: The defending team must retreat a distance of seven (7) metres or to the Defending Try Line as indicated by the Referee after a touch.<|end_of_turn|>\n",
            "Correct Answer: 7 metres.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many substitutions are allowed during a game?<|end_of_turn|> GPT4 Correct Assistant: In touch rugby, there is no limit to the number of substitutions allowed during a game.<|end_of_turn|>\n",
            "Correct Answer: There is no limit.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how long is half time?<|end_of_turn|> GPT4 Correct Assistant: Half time in touch rugby is 5 minutes long.<|end_of_turn|>\n",
            "Correct Answer: 5 minutes.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how does the game commence?<|end_of_turn|> GPT4 Correct Assistant: The game commences with a Tap from the center of the Halfway Line by the player in Possession.<|end_of_turn|>\n",
            "Correct Answer: The game begins with a tap on the halfway line.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many metres must defenders retreat when there is a penalty? Is the same as after a touch is made?<|end_of_turn|> GPT4 Correct Assistant: No, the distance defenders must retreat after a penalty is different from the distance they must retreat after a touch is made.<|end_of_turn|>\n",
            "Correct Answer: 10 metres.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many touches is a team entitled to prior to a change in possession?<|end_of_turn|> GPT4 Correct Assistant: A team is entitled to six (6) Touches prior to a Change of Possession in touch rugby.<|end_of_turn|>\n",
            "Correct Answer: Possession changes on the sixth (6th) touch.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, what happens if a player makes a pass after a touch has been made?<|end_of_turn|> GPT4 Correct Assistant: If a player makes a pass after a touch has been made, the pass is considered dead, and the next touch is zero (0) touch.<|end_of_turn|>\n",
            "Correct Answer: The defending team gains possession and a penalty.\n",
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "input_ids are on: cuda:0\n",
            "<s> GPT4 Correct User: In touch rugby, how many points is a try worth?<|end_of_turn|> GPT4 Correct Assistant: A try is worth one (1) point in touch rugby.<|end_of_turn|>\n",
            "Correct Answer: 1 point.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1e9088a4f324e8d8cb1af0cca33662a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31e492c832284c8db0ff9565429ea773",
              "IPY_MODEL_f3b55eb347b34c929b4d38eadccd9bef",
              "IPY_MODEL_3e8e4db6711c4644a07105b0c9c78a25"
            ],
            "layout": "IPY_MODEL_1484dfce0fd248fcb09999efbc31bf82"
          }
        },
        "31e492c832284c8db0ff9565429ea773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae05589cfa924a539a8ade0a544c1f41",
            "placeholder": "​",
            "style": "IPY_MODEL_a3437e178b5b475bb7a0a0eabc34eac4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f3b55eb347b34c929b4d38eadccd9bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17876e9c41df4859879d416b6ff86cea",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26838ef09f444faa8ca9f40a020deaf9",
            "value": 2
          }
        },
        "3e8e4db6711c4644a07105b0c9c78a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e6051d71e44bdca60bdbd170a8bce6",
            "placeholder": "​",
            "style": "IPY_MODEL_8279474e9ed9463399dea7a3d52f8d5b",
            "value": " 2/2 [00:08&lt;00:00,  3.76s/it]"
          }
        },
        "1484dfce0fd248fcb09999efbc31bf82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae05589cfa924a539a8ade0a544c1f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3437e178b5b475bb7a0a0eabc34eac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17876e9c41df4859879d416b6ff86cea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26838ef09f444faa8ca9f40a020deaf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87e6051d71e44bdca60bdbd170a8bce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8279474e9ed9463399dea7a3d52f8d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ed7922d778044228575174731187884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86ee9c6e5d2b4eea80fdc1db1a37fadf",
              "IPY_MODEL_a4e717033ec34b8c8972db133f6924ff",
              "IPY_MODEL_e6e86e8eb51949c5b415481c6b804581"
            ],
            "layout": "IPY_MODEL_705db3c14c1a42869d754c3e8f78152f"
          }
        },
        "86ee9c6e5d2b4eea80fdc1db1a37fadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d21393569b14ab5ade2aa999f65be3d",
            "placeholder": "​",
            "style": "IPY_MODEL_7feaaeed92d649538f28ce49933651d0",
            "value": "Map: 100%"
          }
        },
        "a4e717033ec34b8c8972db133f6924ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec2a0dcf29c244238757cf8dc95abf76",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c791b05dcb564bc19faedf3714bdd445",
            "value": 60
          }
        },
        "e6e86e8eb51949c5b415481c6b804581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f0a207530454576837139c97e13a2f9",
            "placeholder": "​",
            "style": "IPY_MODEL_55f16a27eec94785b7cb487b95d0c30f",
            "value": " 60/60 [00:00&lt;00:00, 1632.24 examples/s]"
          }
        },
        "705db3c14c1a42869d754c3e8f78152f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d21393569b14ab5ade2aa999f65be3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7feaaeed92d649538f28ce49933651d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec2a0dcf29c244238757cf8dc95abf76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c791b05dcb564bc19faedf3714bdd445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f0a207530454576837139c97e13a2f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55f16a27eec94785b7cb487b95d0c30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}